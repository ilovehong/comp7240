{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Recommendation System\n",
    "\n",
    "This notebook outlines the process of building a recommendation system using Yelp's dataset. The system preprocesses review data to extract features from businesses and users, applies PCA for dimensionality reduction, and evaluates the recommendation quality using nDCG scores. We start by suppressing warnings and importing necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the essential libraries for data manipulation and numerical operations.\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Data\n",
    "\n",
    "Here, we load the business and review datasets previously preprocessed and saved as CSV files. These datasets contain crucial information for feature extraction and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = pd.read_csv('business.csv') \n",
    "review = pd.read_csv('review.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Splitting the dataset into training and testing sets ensures that we can evaluate our recommendation system's performance on unseen data. This step involves random shuffling and splitting the review data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train:test ratio: 9.00\n"
     ]
    }
   ],
   "source": [
    "# Establishing a random seed for consistent shuffling\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly shuffling reviews\n",
    "shuffled_indices = np.random.permutation(review.index)\n",
    "\n",
    "# Determining the split point for training and testing\n",
    "split_idx = int(len(shuffled_indices) * 0.9)\n",
    "\n",
    "# Dividing the data into training and testing sets\n",
    "train = review.loc[shuffled_indices[:split_idx]]\n",
    "test = review.loc[shuffled_indices[split_idx:]]\n",
    "\n",
    "# Displaying the ratio of training to testing data\n",
    "print(f\"Current train:test ratio: {len(train) / len(test):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train:test ratio: 12.06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identifying users exclusive to the test set\n",
    "users_test_only = test[~test.user_id.isin(train.user_id)].user_id.unique()\n",
    "\n",
    "# Choosing one review per user to add to the training set for inclusivity\n",
    "rows_to_add = test[test.user_id.isin(users_test_only)].groupby('user_id').head(1).index\n",
    "\n",
    "# Updating the training and testing sets accordingly\n",
    "idx_train = pd.Index(train.index.tolist() + rows_to_add.tolist()).unique()\n",
    "idx_test = test.index.difference(rows_to_add)\n",
    "\n",
    "train = review.loc[idx_train]\n",
    "test = review.loc[idx_test]\n",
    "\n",
    "# Reporting the updated training to testing data ratio\n",
    "print(f\"Current train:test ratio: {len(train) / len(test):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying businesses unique to the test set\n",
    "businesses_test_only = test[~test.business_id.isin(train.business_id)].business_id.unique()\n",
    "\n",
    "# Selecting one review per such business to move to the training set\n",
    "indices_to_move = test[test.business_id.isin(businesses_test_only)].groupby('business_id').head(1).index\n",
    "\n",
    "# Reallocating indices for training and testing datasets\n",
    "idx_train = train.index.union(indices_to_move)\n",
    "idx_test = test.index.difference(indices_to_move)\n",
    "\n",
    "train = review.loc[idx_train]\n",
    "test = review.loc[idx_test]\n",
    "\n",
    "# Displaying the new training to testing ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_count</th>\n",
       "      <th>review_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0iIxySkp97WNlwK66OGWg</td>\n",
       "      <td>221</td>\n",
       "      <td>I really love this location, it's on the downt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  review_count  \\\n",
       "0  -0iIxySkp97WNlwK66OGWg           221   \n",
       "\n",
       "                                     review_combined  \n",
       "0  I really love this location, it's on the downt...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_by_rest = train.groupby('business_id').agg(\n",
    "    review_count=('review_id', 'count'), \n",
    "    review_combined=('text', lambda x: '###'.join(x))\n",
    ").reset_index()\n",
    "\n",
    "# Checking the aggregation result\n",
    "rev_by_rest.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Reviews\n",
    "\n",
    "Feature extraction is performed on the review texts by grouping them by businesses and applying TF-IDF vectorization. This process converts text data into a matrix of TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100' '13' '14' '16' '18' '19' '20 minutes' '24' '30 minutes' '40' '45'\n",
      " '45 minutes' '95' '99' 'absolute' 'absolutely delicious' 'accommodating'\n",
      " 'actual' 'addition' 'additional' 'adults' 'affordable' 'afternoon' 'ahi'\n",
      " 'air' 'al' 'amazing food' 'amazing service' 'ambiance' 'ambience'\n",
      " 'american' 'anniversary' 'answer' 'anymore' 'apart' 'apologized' 'app'\n",
      " 'apparently' 'appetizer' 'appetizers' 'apple' 'appreciate' 'appreciated'\n",
      " 'arrive' 'asada' 'asian' 'atlantis' 'authentic' 'avocado' 'avoid']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization for feature extraction from concatenated reviews\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=1000,\n",
    "    max_df=0.5,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(rev_by_rest.review_combined)\n",
    "\n",
    "# Inspecting the most significant features\n",
    "top_features = vectorizer.get_feature_names_out()[:50]\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20 minutes</th>\n",
       "      <th>24</th>\n",
       "      <th>30 minutes</th>\n",
       "      <th>40</th>\n",
       "      <th>...</th>\n",
       "      <th>wrap</th>\n",
       "      <th>wrapped</th>\n",
       "      <th>write</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year old</th>\n",
       "      <th>years ago</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>yum</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-0iIxySkp97WNlwK66OGWg</th>\n",
       "      <td>0.002297</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>0.00387</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.002428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1YvpVvnnLrTZ0zjtUYPXA</th>\n",
       "      <td>0</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.017318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010869</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.02555</td>\n",
       "      <td>0.017116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3xX_IfttKjPJ792BOBJ-Q</th>\n",
       "      <td>0.004554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>0.00504</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.017756</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018928</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.014439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-7KnD-G4ZYi7-Xs4ZJAYWQ</th>\n",
       "      <td>0.010939</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.01287</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Dr6MZW6ZVP7X6ai30Qrlw</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012143</td>\n",
       "      <td>0.013558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             100        13        14        16        18  \\\n",
       "-0iIxySkp97WNlwK66OGWg  0.002297         0         0  0.005097   0.00387   \n",
       "-1YvpVvnnLrTZ0zjtUYPXA         0  0.005898  0.017318         0         0   \n",
       "-3xX_IfttKjPJ792BOBJ-Q  0.004554         0         0  0.005052  0.015344   \n",
       "-7KnD-G4ZYi7-Xs4ZJAYWQ  0.010939  0.002988         0  0.003034  0.003071   \n",
       "-Dr6MZW6ZVP7X6ai30Qrlw         0         0         0         0  0.014408   \n",
       "\n",
       "                              19  20 minutes        24  30 minutes        40  \\\n",
       "-0iIxySkp97WNlwK66OGWg  0.002542    0.002146  0.001332    0.001156  0.002162   \n",
       "-1YvpVvnnLrTZ0zjtUYPXA  0.005974           0         0    0.010869         0   \n",
       "-3xX_IfttKjPJ792BOBJ-Q   0.00504    0.034028  0.005281    0.027508  0.008573   \n",
       "-7KnD-G4ZYi7-Xs4ZJAYWQ  0.003026    0.017879         0    0.008259   0.01287   \n",
       "-Dr6MZW6ZVP7X6ai30Qrlw  0.014197           0         0           0         0   \n",
       "\n",
       "                        ...      wrap   wrapped     write      yeah  year old  \\\n",
       "-0iIxySkp97WNlwK66OGWg  ...  0.002842         0  0.004241   0.00112         0   \n",
       "-1YvpVvnnLrTZ0zjtUYPXA  ...         0         0         0         0  0.006119   \n",
       "-3xX_IfttKjPJ792BOBJ-Q  ...         0  0.010643  0.004203  0.017756         0   \n",
       "-7KnD-G4ZYi7-Xs4ZJAYWQ  ...         0         0         0  0.010662         0   \n",
       "-Dr6MZW6ZVP7X6ai30Qrlw  ...         0         0         0         0         0   \n",
       "\n",
       "                        years ago  yesterday      york       yum      zero  \n",
       "-0iIxySkp97WNlwK66OGWg   0.002387          0  0.008615  0.002175  0.002428  \n",
       "-1YvpVvnnLrTZ0zjtUYPXA          0   0.005297         0   0.02555  0.017116  \n",
       "-3xX_IfttKjPJ792BOBJ-Q   0.018928   0.013405         0  0.017243  0.014439  \n",
       "-7KnD-G4ZYi7-Xs4ZJAYWQ   0.002842          0         0         0         0  \n",
       "-Dr6MZW6ZVP7X6ai30Qrlw    0.01333          0         0  0.012143  0.013558  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transforming the TF-IDF sparse matrix to a DataFrame\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "rest_revfeature = pd.DataFrame.sparse.from_spmatrix(X, columns=feature_names)\n",
    "\n",
    "# Aligning the business_id with the index\n",
    "rest_revfeature.index = rev_by_rest.business_id.values\n",
    "rest_revfeature.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying PCA for Dimensionality Reduction\n",
    "\n",
    "To reduce the dimensionality of the TF-IDF feature matrix and capture the essence of the dataset, PCA (Principal Component Analysis) is applied. This transformation retains components that explain a significant portion of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pca_components</th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_120</th>\n",
       "      <th>PCA_121</th>\n",
       "      <th>PCA_122</th>\n",
       "      <th>PCA_123</th>\n",
       "      <th>PCA_124</th>\n",
       "      <th>PCA_125</th>\n",
       "      <th>PCA_126</th>\n",
       "      <th>PCA_127</th>\n",
       "      <th>PCA_128</th>\n",
       "      <th>PCA_129</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-0iIxySkp97WNlwK66OGWg</th>\n",
       "      <td>-0.0178</td>\n",
       "      <td>-0.066374</td>\n",
       "      <td>0.041662</td>\n",
       "      <td>0.177428</td>\n",
       "      <td>0.074457</td>\n",
       "      <td>0.142361</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.130991</td>\n",
       "      <td>0.029388</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019519</td>\n",
       "      <td>-0.005879</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>-0.007785</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.00089</td>\n",
       "      <td>-0.006741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "pca_components           PCA_1     PCA_2     PCA_3     PCA_4     PCA_5  \\\n",
       "-0iIxySkp97WNlwK66OGWg -0.0178 -0.066374  0.041662  0.177428  0.074457   \n",
       "\n",
       "pca_components             PCA_6     PCA_7     PCA_8     PCA_9    PCA_10  ...  \\\n",
       "-0iIxySkp97WNlwK66OGWg  0.142361  0.029258  0.130991  0.029388  0.011707  ...   \n",
       "\n",
       "pca_components           PCA_120   PCA_121   PCA_122   PCA_123   PCA_124  \\\n",
       "-0iIxySkp97WNlwK66OGWg -0.019519 -0.005879 -0.004544  0.007208 -0.007785   \n",
       "\n",
       "pca_components           PCA_125   PCA_126   PCA_127  PCA_128   PCA_129  \n",
       "-0iIxySkp97WNlwK66OGWg  0.005574 -0.005158  0.005407  0.00089 -0.006741  \n",
       "\n",
       "[1 rows x 129 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA_120</th>\n",
       "      <th>PCA_121</th>\n",
       "      <th>PCA_122</th>\n",
       "      <th>PCA_123</th>\n",
       "      <th>PCA_124</th>\n",
       "      <th>PCA_125</th>\n",
       "      <th>PCA_126</th>\n",
       "      <th>PCA_127</th>\n",
       "      <th>PCA_128</th>\n",
       "      <th>PCA_129</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>--3Hl2oAvTPlq-f7KtogJg</th>\n",
       "      <td>-0.02244</td>\n",
       "      <td>-0.036778</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>-0.095409</td>\n",
       "      <td>0.055039</td>\n",
       "      <td>-0.015558</td>\n",
       "      <td>0.152621</td>\n",
       "      <td>-0.144908</td>\n",
       "      <td>0.07329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.014476</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>0.004826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          PCA_1     PCA_2     PCA_3     PCA_4     PCA_5  \\\n",
       "user_id                                                                   \n",
       "--3Hl2oAvTPlq-f7KtogJg -0.02244 -0.036778  0.102362  0.053947 -0.095409   \n",
       "\n",
       "                           PCA_6     PCA_7     PCA_8     PCA_9   PCA_10  ...  \\\n",
       "user_id                                                                  ...   \n",
       "--3Hl2oAvTPlq-f7KtogJg  0.055039 -0.015558  0.152621 -0.144908  0.07329  ...   \n",
       "\n",
       "                         PCA_120   PCA_121   PCA_122   PCA_123   PCA_124  \\\n",
       "user_id                                                                    \n",
       "--3Hl2oAvTPlq-f7KtogJg  0.006327  0.002786  0.009739 -0.007246 -0.003631   \n",
       "\n",
       "                         PCA_125   PCA_126   PCA_127   PCA_128   PCA_129  \n",
       "user_id                                                                   \n",
       "--3Hl2oAvTPlq-f7KtogJg  0.011568  0.014476  0.010554 -0.007374  0.004826  \n",
       "\n",
       "[1 rows x 129 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Executing PCA to capture 80% of the variance\n",
    "pca = PCA(n_components=0.8)\n",
    "rest_pcafeature = pca.fit_transform(rest_revfeature)\n",
    "\n",
    "# Defining component names dynamically based on PCA results\n",
    "num_components = pca.n_components_\n",
    "rest_pcafeature_df = pd.DataFrame(rest_pcafeature, \n",
    "                                  index=rest_revfeature.index, \n",
    "                                  columns=[f'PCA_{i}' for i in range(1, num_components + 1)])\n",
    "rest_pcafeature_df.columns.name = 'pca_components'\n",
    "\n",
    "# Normalizing and merging PCA features with user preferences for profile construction\n",
    "\n",
    "rest_pcafeature_df = rest_pcafeature_df.div(np.linalg.norm(rest_pcafeature_df, axis=1), axis=0)\n",
    "user_prefs = pd.merge(train[['user_id', 'business_id', 'stars']], \n",
    "                      rest_pcafeature_df, \n",
    "                      how='inner', \n",
    "                      left_on='business_id', \n",
    "                      right_index=True).drop(columns=['business_id'])\n",
    "\n",
    "for i in range(1, num_components + 1):\n",
    "    user_prefs[f'PCA_{i}'] *= user_prefs['stars']\n",
    "\n",
    "user_pcafeature_df = user_prefs.groupby('user_id').sum().drop(columns='stars')\n",
    "user_pcafeature_df = user_pcafeature_df.div(np.linalg.norm(user_pcafeature_df, axis=1), axis=0)\n",
    "\n",
    "# Displaying the initial entries for both restaurants and users after PCA transformation\n",
    "display(rest_pcafeature_df.head(1))\n",
    "display(user_pcafeature_df.head(1))\n",
    "\n",
    "# Save the restaurant and user PCA features\n",
    "rest_pcafeature_df.to_pickle('rest_pcafeature_train.pkl')\n",
    "user_pcafeature_df.to_pickle('user_pcafeature_train.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System Evaluation\n",
    "\n",
    "The recommendation system's performance is evaluated using the normalized Discounted Cumulative Gain (nDCG) metric. This evaluation considers the relevance of recommended businesses to the users' preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG Score: 0.559138825007025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def load_pca_features():\n",
    "    # Load restaurant PCA features\n",
    "    with open('rest_pcafeature_train.pkl', 'rb') as f:\n",
    "        rest_pcafeature = pickle.load(f)\n",
    "        \n",
    "    # Load user PCA features\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize('user_pcafeature_train.pkl')\n",
    "    with open('user_pcafeature_train.pkl', 'rb') as f:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f.read(max_bytes)\n",
    "        user_pcafeature = pickle.loads(bytes_in)\n",
    "    \n",
    "    return user_pcafeature, rest_pcafeature\n",
    "\n",
    "def dcg_at_k(scores):\n",
    "    \"\"\"Calculate DCG\"\"\"\n",
    "    return np.sum(\n",
    "        (2**scores - 1) / np.log2(np.arange(2, scores.size + 2))\n",
    "    )\n",
    "\n",
    "def ndcg_at_k(true_scores, pred_scores, k):\n",
    "    \"\"\"Calculate nDCG\"\"\"\n",
    "    best_dcg = dcg_at_k(np.sort(true_scores)[::-1][:k])\n",
    "    if best_dcg == 0:\n",
    "        return 0\n",
    "    return dcg_at_k(pred_scores[:k]) / best_dcg\n",
    "\n",
    "def evaluate_ndcg(user_pcafeature, rest_pcafeature, test_data, k=10):\n",
    "    ndcg_scores = []\n",
    "    \n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        if user_id in user_pcafeature.index:\n",
    "            sim_matrix = linear_kernel(user_pcafeature.loc[[user_id]], rest_pcafeature).flatten()\n",
    "            predictions = pd.Series(sim_matrix, index=rest_pcafeature.index).sort_values(ascending=False)\n",
    "\n",
    "            # Filter to the business_ids in test_data for this user\n",
    "            true_relevance = test_data[test_data['user_id'] == user_id]\n",
    "            # Ensure business_id is unique in true_relevance\n",
    "            if not true_relevance['business_id'].is_unique:\n",
    "                # Handle duplicates as needed, for example by averaging duplicate entries\n",
    "                true_relevance = true_relevance.groupby('business_id')['stars'].mean().reset_index()\n",
    "            true_relevance = true_relevance.set_index('business_id')['stars'].reindex(predictions.index).fillna(0)\n",
    "            \n",
    "            # Calculate nDCG\n",
    "            ndcg_score = ndcg_at_k(true_relevance.values, predictions.values, k)\n",
    "            ndcg_scores.append(ndcg_score)\n",
    "    \n",
    "    # Average nDCG score across all users\n",
    "    avg_ndcg = np.mean(ndcg_scores)\n",
    "    return avg_ndcg\n",
    "\n",
    "# Assuming you have test_data DataFrame with columns ['user_id', 'business_id', 'stars']\n",
    "user_pcafeature, rest_pcafeature = load_pca_features()\n",
    "avg_ndcg_score = evaluate_ndcg(user_pcafeature, rest_pcafeature, test)\n",
    "print(f\"Average nDCG Score: {avg_ndcg_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving PCA Features and Profiles\n",
    "\n",
    "After transforming the business review features and user preferences into lower-dimensional spaces using PCA, these new feature sets are saved for future use in recommendation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle  # Ensure pickle is imported for serialization\n",
    "\n",
    "# Assuming `review` is defined elsewhere in your code and contains the review data\n",
    "\n",
    "# Group reviews by business_id, concatenate text, and count reviews\n",
    "rev_by_rest = review.groupby('business_id').agg(\n",
    "    review_count=('review_id', 'count'),\n",
    "    review_combined=('text', lambda texts: '###'.join(texts))\n",
    ").reset_index()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000)\n",
    "X = vectorizer.fit_transform(rev_by_rest.review_combined)\n",
    "\n",
    "# The feature names from TF-IDF\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame with TF-IDF features\n",
    "rest_revfeature = pd.DataFrame(X.toarray(), index=rev_by_rest.business_id, columns=feature_names)\n",
    "\n",
    "# Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=0.80)  # Keep components that explain 80% of the variance\n",
    "rest_pcafeature = pca.fit_transform(rest_revfeature)\n",
    "rest_pcafeature_df = pd.DataFrame(rest_pcafeature, index=rest_revfeature.index)\n",
    "\n",
    "# Get the original feature names corresponding to the PCA components\n",
    "original_feature_names = np.array(feature_names)[np.argsort(np.abs(pca.components_), axis=1)[:, ::-1]]\n",
    "\n",
    "\n",
    "# Normalize restaurant feature vectors\n",
    "rest_pcafeature_df = rest_pcafeature_df.div(np.linalg.norm(rest_pcafeature_df, axis=1), axis=0)\n",
    "\n",
    "# Merge user ratings with restaurant PCA features\n",
    "user_pcafeature = pd.merge(\n",
    "    review[['user_id', 'business_id', 'stars']],\n",
    "    rest_pcafeature_df,\n",
    "    how='inner',\n",
    "    left_on='business_id',\n",
    "    right_index=True\n",
    ").drop('business_id', axis=1)\n",
    "\n",
    "# Scale PCA components by user ratings\n",
    "for col in user_pcafeature.columns[2:]:  # Skip user_id and stars columns\n",
    "    user_pcafeature[col] = user_pcafeature[col] * user_pcafeature['stars']\n",
    "\n",
    "# Aggregate PCA components by user\n",
    "user_pcafeature_df = user_pcafeature.groupby('user_id').sum().drop(columns='stars')\n",
    "\n",
    "# Normalize user feature vectors\n",
    "user_pcafeature_df = user_pcafeature_df.div(np.linalg.norm(user_pcafeature_df, axis=1), axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the restaurant and user PCA features\n",
    "# Save the restaurant and user PCA features\n",
    "rest_pcafeature_df.to_pickle('rest_pcafeature_all.pkl')\n",
    "user_pcafeature_df.to_pickle('user_pcafeature_all.pkl')\n",
    "\n",
    "with open('./model/original_feature_names.pickle', 'wb') as f:\n",
    "    pickle.dump(original_feature_names, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup and Data Loading\n",
    "\n",
    "In this section, we import necessary libraries and load the Yelp dataset for businesses and reviews. This includes setting up visualization preferences for consistent and appealing plots, and ensuring our data is ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "business = pd.read_csv('business.csv')\n",
    "review = pd.read_csv('review.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting and Preparation\n",
    "\n",
    "This section covers the shuffling and splitting of our dataset into training and testing sets. We also detail the process of preparing these sets for the recommendation system's model training phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, GridSearchCV, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deduplicate reviews to ensure each user-business pair is unique\n",
    "unique_reviews = review.drop_duplicates(['user_id', 'business_id'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Set a random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle and split the dataset: 90% for training and 10% for testing\n",
    "shuffled_indices = np.random.permutation(unique_reviews.index)\n",
    "split_idx = int(0.9 * len(shuffled_indices))\n",
    "train_indices, test_indices = shuffled_indices[:split_idx], shuffled_indices[split_idx:]\n",
    "\n",
    "# Function to select relevant columns and convert 'stars' to numeric, removing any nulls\n",
    "def prepare_dataset(df, indices):\n",
    "    subset = df.iloc[indices][['user_id', 'business_id', 'stars']]\n",
    "    subset['stars'] = pd.to_numeric(subset['stars'], errors='coerce').dropna()\n",
    "    return subset\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "trainset = prepare_dataset(unique_reviews, train_indices)\n",
    "testset = prepare_dataset(unique_reviews, test_indices)\n",
    "\n",
    "# Initialize a Reader with the rating scale and load the training dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_train = Dataset.load_from_df(trainset, reader)\n",
    "training = data_train.build_full_trainset()\n",
    "\n",
    "# Prepare the testing dataset as a list of (user, item, rating) tuples\n",
    "testing = [(uid, iid, float(r)) for uid, iid, r in testset.itertuples(index=False, name=None)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We train our model using the Singular Value Decomposition (SVD) algorithm, evaluate its performance through RMSE, and extract insights from the latent feature matrices. This step is crucial for understanding the effectiveness of our recommendation system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.2719\n",
      "RMSE (with bias): 1.271863541367604\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# The unique_reviews DataFrame contains deduplicated reviews with columns: 'user_id', 'business_id', and 'stars'\n",
    "# Initializing the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(unique_reviews[['user_id', 'business_id', 'stars']], reader)\n",
    "\n",
    "# Configuring and training the SVD model incorporating user and item biases by default\n",
    "svd_biased = SVD(n_factors=10, n_epochs=20, biased=True, random_state=42)\n",
    "svd_biased.fit(training)\n",
    "\n",
    "# Evaluating the model's performance by predicting ratings on the test set and calculating the RMSE\n",
    "predictions_biased = svd_biased.test(testing)\n",
    "rmse_biased = accuracy.rmse(predictions_biased)\n",
    "\n",
    "print(f\"RMSE (with bias): {rmse_biased}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Hyperparameter Tuning\n",
    "\n",
    "Optimizing the SVD model's parameters through cross-validation and GridSearch to achieve better accuracy. This involves adjusting factors like the number of epochs, regularization terms, and the inclusion of bias terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score from grid search: 1.2758585387492642\n",
      "Best parameter combination: {'n_factors': 3, 'n_epochs': 40, 'reg_all': 0.1, 'biased': True}\n",
      "RMSE: 1.2657\n",
      "RMSE with optimized parameters: 1.2657443744412762\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import NMF, SVD\n",
    "from surprise.model_selection import cross_validate, GridSearchCV, KFold\n",
    "from surprise import accuracy\n",
    "\n",
    "# Conducting parameter optimization for the SVD algorithm with bias using scikit-surprise's GridSearchCV.\n",
    "# This step is crucial for enhancing model accuracy by finding the optimal set of hyperparameters.\n",
    "\n",
    "# Defining the parameter grid for SVD hyperparameter tuning.\n",
    "param_grid = {\n",
    "    'n_factors': [3, 10],  # Number of factors\n",
    "    'n_epochs': [20, 40, 60],  # Number of iterations of the SGD procedure\n",
    "    'reg_all': [0.05, 0.1],  # Regularization term for all parameters\n",
    "    'biased': [True]  # Use the baseline estimates in the algorithm\n",
    "}\n",
    "\n",
    "# Initializing GridSearchCV with the SVD algorithm, specified parameter grid, and cross-validation settings.\n",
    "svd_gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=KFold(3, random_state=42), joblib_verbose=2)\n",
    "\n",
    "# Fitting GridSearchCV to the training data to find the best SVD model parameters.\n",
    "svd_gs.fit(data_train)\n",
    "\n",
    "# Displaying the best RMSE score achieved during the optimization process.\n",
    "print(\"Best RMSE score from grid search:\", svd_gs.best_score['rmse'])\n",
    "\n",
    "# Showing the combination of parameters that achieved the best RMSE score.\n",
    "print(\"Best parameter combination:\", svd_gs.best_params['rmse'])\n",
    "\n",
    "# Retrieving the best SVD model from the grid search and fitting it to the training data.\n",
    "svd_gs_best = svd_gs.best_estimator['rmse']\n",
    "svd_gs_best.fit(training)\n",
    "\n",
    "# Making predictions on the test set with the optimized model and evaluating the RMSE.\n",
    "pred_svd_gs_best = svd_gs_best.test(testing)\n",
    "rmse_optimized = accuracy.rmse(pred_svd_gs_best)\n",
    "\n",
    "print(f\"RMSE with optimized parameters: {rmse_optimized}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Discounted Cumulative Gain (NDCG) Evaluation\n",
    "\n",
    "This section focuses on evaluating our recommendation system using the Normalized Discounted Cumulative Gain (NDCG) metric. NDCG is a standard measure in information retrieval and recommendation systems to quantify the effectiveness of our ranking algorithms. By computing NDCG, we can assess how well our system ranks items in a way that aligns with the user's preferences, with particular emphasis on the importance of the order in which items are presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset\n",
    "\n",
    "# Applying the optimized SVD model, `svd_biased`, to generate predictions for the test dataset.\n",
    "predictions = svd_gs_best.test(testing)\n",
    "\n",
    "# Organizing the predictions for each user into a dictionary for easier manipulation.\n",
    "user_pred = defaultdict(list)\n",
    "for uid, iid, true_r, est, _ in predictions:\n",
    "    user_pred[uid].append((iid, est))\n",
    "\n",
    "# Defining functions to calculate Discounted Cumulative Gain (DCG) and Normalized DCG (NDCG).\n",
    "def dcg_at_k(ranked_list, k):\n",
    "    \"\"\"Calculate Discounted Cumulative Gain at rank k.\"\"\"\n",
    "    ranked_list = np.asfarray(ranked_list)[:k]\n",
    "    if ranked_list.size:\n",
    "        return np.sum(ranked_list / np.log2(np.arange(2, ranked_list.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(ranked_list, k):\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain at rank k.\"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(ranked_list, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(ranked_list, k) / dcg_max\n",
    "\n",
    "# Setting the rank threshold to evaluate the top 10 recommended items.\n",
    "k = 10\n",
    "ndcg_scores = []\n",
    "\n",
    "# Computing NDCG for each user based on their predicted ratings.\n",
    "for uid, user_ratings in user_pred.items():\n",
    "    user_ratings.sort(key=lambda x: x[1], reverse=True)  # Sort ratings in descending order.\n",
    "    est_rank = [rating for iid, rating in user_ratings]  # Extract estimated ranks.\n",
    "    ndcg = ndcg_at_k(est_rank, k)  # Compute NDCG for the current user.\n",
    "    ndcg_scores.append(ndcg)\n",
    "\n",
    "# Calculating the average NDCG across all users to measure overall system performance.\n",
    "avg_ndcg = np.mean(ndcg_scores)\n",
    "print(f\"Average NDCG@{k}: {avg_ndcg}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

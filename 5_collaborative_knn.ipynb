{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup and Data Loading\n",
    "\n",
    "In this section, we import necessary libraries and load the Yelp dataset for businesses and reviews. This includes setting up visualization preferences for consistent and appealing plots, and ensuring our data is ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "business = pd.read_csv('business.csv')\n",
    "review = pd.read_csv('review.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting and Preparation\n",
    "\n",
    "This section covers the shuffling and splitting of our dataset into training and testing sets. We also detail the process of preparing these sets for the recommendation system's model training phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate, GridSearchCV, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deduplicate reviews to ensure each user-business pair is unique\n",
    "unique_reviews = review.drop_duplicates(['user_id', 'business_id'], keep='first').reset_index(drop=True)\n",
    "\n",
    "# Set a random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle and split the dataset: 90% for training and 10% for testing\n",
    "shuffled_indices = np.random.permutation(unique_reviews.index)\n",
    "split_idx = int(0.9 * len(shuffled_indices))\n",
    "train_indices, test_indices = shuffled_indices[:split_idx], shuffled_indices[split_idx:]\n",
    "\n",
    "# Function to select relevant columns and convert 'stars' to numeric, removing any nulls\n",
    "def prepare_dataset(df, indices):\n",
    "    subset = df.iloc[indices][['user_id', 'business_id', 'stars']]\n",
    "    subset['stars'] = pd.to_numeric(subset['stars'], errors='coerce').dropna()\n",
    "    return subset\n",
    "\n",
    "# Prepare the training and testing datasets\n",
    "trainset = prepare_dataset(unique_reviews, train_indices)\n",
    "testset = prepare_dataset(unique_reviews, test_indices)\n",
    "\n",
    "# Initialize a Reader with the rating scale and load the training dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_train = Dataset.load_from_df(trainset, reader)\n",
    "training = data_train.build_full_trainset()\n",
    "\n",
    "# Prepare the testing dataset as a list of (user, item, rating) tuples\n",
    "testing = [(uid, iid, float(r)) for uid, iid, r in testset.itertuples(index=False, name=None)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We train our model using the KNNWithMeans item-based algorithm, evaluate its performance through RMSE, and extract insights from the latent feature matrices. This step is crucial for understanding the effectiveness of our recommendation system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.3908\n",
      "RMSE (with bias): 1.390791127171579\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# The unique_reviews DataFrame contains deduplicated reviews with columns: 'user_id', 'business_id', and 'stars'\n",
    "# Initializing the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(unique_reviews[['user_id', 'business_id', 'stars']], reader)\n",
    "\n",
    "# Configuring and training the KNNWithMeans model incorporating user and item biases by default\n",
    "knn = KNNWithMeans(k=80, sim_options={\n",
    "      \"name\": \"cosine\",\n",
    "      \"user_based\": False,  \n",
    "    })\n",
    "knn.fit(training)\n",
    "\n",
    "# Evaluating the model's performance by predicting ratings on the test set and calculating the RMSE\n",
    "predictions_biased = knn.test(testing)\n",
    "rmse_biased = accuracy.rmse(predictions_biased)\n",
    "\n",
    "print(f\"RMSE (with bias): {rmse_biased}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Hyperparameter Tuning\n",
    "\n",
    "Optimizing the KNNWithMeans model's parameters through cross-validation and GridSearch to achieve better accuracy. This involves adjusting factors like the number of epochs, regularization terms, and the inclusion of bias terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Best RMSE score from grid search: 1.4120034999794935\n",
      "Best parameter combination: {'k': 80, 'sim_options': {'name': 'cosine', 'user_based': False}}\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.3908\n",
      "RMSE with optimized parameters: 1.390791127171579\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import NMF, SVD\n",
    "from surprise.model_selection import cross_validate, GridSearchCV, KFold\n",
    "from surprise import accuracy\n",
    "\n",
    "# Conducting parameter optimization for the KNNWithMeans algorithm with bias using scikit-surprise's GridSearchCV.\n",
    "# This step is crucial for enhancing model accuracy by finding the optimal set of hyperparameters.\n",
    "\n",
    "# Defining the parameter grid for KNNWithMeans hyperparameter tuning.\n",
    "param_grid = {\n",
    "    'k': [10, 20, 40, 80],  # Different values for k\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson'],  # Different similarity metrics\n",
    "        'user_based': [False]  # Item-based \n",
    "    }\n",
    "}\n",
    "\n",
    "knn_gs = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse', 'mae'], cv=KFold(3, random_state=42), joblib_verbose=2)\n",
    "\n",
    "# Fitting GridSearchCV to the training data to find the best KNNWithMeans model parameters.\n",
    "knn_gs.fit(data_train)\n",
    "\n",
    "# Displaying the best RMSE score achieved during the optimization process.\n",
    "print(\"Best RMSE score from grid search:\", knn_gs.best_score['rmse'])\n",
    "\n",
    "# Showing the combination of parameters that achieved the best RMSE score.\n",
    "print(\"Best parameter combination:\", knn_gs.best_params['rmse'])\n",
    "\n",
    "# Retrieving the best KNNWithMeans model from the grid search and fitting it to the training data.\n",
    "knn_gs_best = knn_gs.best_estimator['rmse']\n",
    "knn_gs_best.fit(training)\n",
    "\n",
    "# Making predictions on the test set with the optimized model and evaluating the RMSE.\n",
    "pred_knn_gs_best = knn_gs_best.test(testing)\n",
    "rmse_optimized = accuracy.rmse(pred_knn_gs_best)\n",
    "\n",
    "print(f\"RMSE with optimized parameters: {rmse_optimized}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter combination: {'k': 80, 'sim_options': {'name': 'cosine', 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter combination:\", knn_gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Discounted Cumulative Gain (NDCG) Evaluation\n",
    "\n",
    "This section focuses on evaluating our recommendation system using the Normalized Discounted Cumulative Gain (NDCG) metric. NDCG is a standard measure in information retrieval and recommendation systems to quantify the effectiveness of our ranking algorithms. By computing NDCG, we can assess how well our system ranks items in a way that aligns with the user's preferences, with particular emphasis on the importance of the order in which items are presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NDCG@10: 0.9605759073505854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Assuming svd_gs_best.test(testing) is already defined and produces predictions.\n",
    "predictions = knn_gs_best.test(testing)\n",
    "\n",
    "# Organizing the predictions for each user into a dictionary for easier manipulation.\n",
    "user_true = defaultdict(list)\n",
    "user_pred = defaultdict(list)\n",
    "for uid, _, true_r, est, _ in predictions:\n",
    "    user_true[uid].append(true_r)\n",
    "    user_pred[uid].append(est)\n",
    "\n",
    "k = 10\n",
    "average_ndcg_scores = []\n",
    "\n",
    "for uid in user_true.keys():\n",
    "    # Ensure both true and predicted ratings are sorted by the predicted rating's order\n",
    "    # and limit the length to the top-k items to compare.\n",
    "    temp_true = [true_r for _, true_r in sorted(zip(user_pred[uid], user_true[uid]), reverse=True)[:k]]\n",
    "    temp_pred = sorted(user_pred[uid], reverse=True)[:k]\n",
    "    \n",
    "    # Check if the user has at least k ratings; if not, continue to the next user.\n",
    "    if len(temp_true) < k or len(temp_pred) < k:\n",
    "        continue\n",
    "    \n",
    "    # Calculate NDCG for the current user and append to the list of scores.\n",
    "    user_ndcg_score = ndcg_score([temp_true], [temp_pred])\n",
    "    average_ndcg_scores.append(user_ndcg_score)\n",
    "\n",
    "# Calculate the average NDCG across all users.\n",
    "if average_ndcg_scores:\n",
    "    avg_ndcg = np.mean(average_ndcg_scores)\n",
    "    print(f\"Average NDCG@{k}: {avg_ndcg}\")\n",
    "else:\n",
    "    print(\"Not enough data to calculate average NDCG.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

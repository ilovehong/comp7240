{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Here we import the required libraries and load our datasets. We use `pandas` for data manipulation, `numpy` for numerical operations, and `matplotlib` for plotting. Additionally, we use scikit-learn for encoding categorical variables and splitting the data, and TensorFlow with Keras for building the neural network model. After loading, we pre-process the data by selecting the relevant columns and encoding the user and business IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dot, Concatenate, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_csv('review.csv')\n",
    "business = pd.read_csv('business.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review[['user_id','business_id','stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(review, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding User and Business IDs\n",
    "To handle categorical data in our model, we encode the user and business IDs using label encoding, which converts each unique string into a numerical representation. This is a necessary step before feeding the data into our neural network model. The encoding process is applied to both the training and testing datasets to ensure consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "train_data['user_id_encoded'] = user_encoder.fit_transform(train_data['user_id'])\n",
    "train_data['business_id_encoded'] = business_encoder.fit_transform(train_data['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data['user_id'].isin(user_encoder.classes_)]\n",
    "test_data = test_data[test_data['business_id'].isin(business_encoder.classes_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['user_id_encoded'] = user_encoder.transform(test_data['user_id'])\n",
    "test_data['business_id_encoded'] = business_encoder.transform(test_data['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 75510, Unique Businesses: 1705\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "print(f\"Unique Users: {num_users}, Unique Businesses: {num_businesses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Neural Network Model\n",
    "Here we define our neural network model architecture. We use embeddings to capture the latent factors of users and businesses and concatenate these embeddings to form the input to the dense layers of the network. The model aims to predict user ratings for businesses. We use a mean squared error loss function and the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=32\n",
    "\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "business_input = Input(shape=(1,), name='business_input')\n",
    "\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)\n",
    "business_embedding = Embedding(input_dim=num_businesses, output_dim=embedding_dim, embeddings_regularizer=l2(1e-6))(business_input)\n",
    "\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "business_flatten = Flatten()(business_embedding)\n",
    "\n",
    "merged = Concatenate()([user_flatten, business_flatten])\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "dense_layer = Dense(128, activation='relu')(merged)\n",
    "dropout = Dropout(0.4)(dense_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout)\n",
    "\n",
    "model = Model(inputs=[user_input, business_input], outputs=output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "In this section, we fit the model to our training data using a batch size of 128 and a specified number of epochs. We implement early stopping to prevent overfitting and a model checkpoint to save the best weights during training. We monitor the validation loss and stop training if it doesn't improve after a defined number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "user_ids = train_data['user_id_encoded'].values\n",
    "business_ids = train_data['business_id_encoded'].values\n",
    "stars = train_data['stars'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(f'./model/model.weights.h5',\n",
    "                             monitor='val_loss',   # Monitor validation loss\n",
    "                             save_best_only=True,  # Save only the best model\n",
    "                             save_weights_only=True,\n",
    "                             mode='min'            # Mode of monitoring (minimize validation loss)\n",
    "                            )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=5,\n",
    "                               restore_best_weights=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 3.7999 - mae: 1.5459 - val_loss: 1.7116 - val_mae: 1.0588\n",
      "Epoch 2/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 1.5151 - mae: 0.9889 - val_loss: 1.7550 - val_mae: 1.0786\n",
      "Epoch 3/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 1.0752 - mae: 0.8187 - val_loss: 1.8031 - val_mae: 1.0868\n",
      "Epoch 4/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.8608 - mae: 0.7226 - val_loss: 1.8230 - val_mae: 1.0957\n",
      "Epoch 5/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.7522 - mae: 0.6709 - val_loss: 1.8052 - val_mae: 1.0844\n",
      "Epoch 6/20\n",
      "\u001b[1m1222/1222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.6619 - mae: 0.6270 - val_loss: 1.8151 - val_mae: 1.0887\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [user_ids, business_ids],\n",
    "    stars,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Once training is complete, we use our test dataset to evaluate the model's performance. We predict the ratings and calculate the mean squared error between the predicted and actual ratings to understand the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_ids = test_data['user_id_encoded'].values\n",
    "test_business_ids = test_data['business_id_encoded'].values\n",
    "test_stars = test_data['stars'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict([test_user_ids, test_business_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.573513826411443"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions,test_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 166 ms, sys: 4.9 ms, total: 171 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "review['user_id_encoded'] = user_encoder.fit_transform(review['user_id'])\n",
    "review['business_id_encoded'] = business_encoder.fit_transform(review['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Users: 87090, Unique Businesses: 1705\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "print(f\"Unique Users: {num_users}, Unique Businesses: {num_businesses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=32\n",
    "\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "business_input = Input(shape=(1,), name='business_input')\n",
    "\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)\n",
    "business_embedding = Embedding(input_dim=num_businesses, output_dim=embedding_dim, embeddings_regularizer=l2(1e-6))(business_input)\n",
    "\n",
    "user_flatten = Flatten()(user_embedding)\n",
    "business_flatten = Flatten()(business_embedding)\n",
    "\n",
    "merged = Concatenate()([user_flatten, business_flatten])\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "dense_layer = Dense(128, activation='relu')(merged)\n",
    "dropout = Dropout(0.4)(dense_layer)\n",
    "output_layer = Dense(1, activation='linear')(dropout)\n",
    "\n",
    "model = Model(inputs=[user_input, business_input], outputs=output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "user_ids = review['user_id_encoded'].values\n",
    "business_ids = review['business_id_encoded'].values\n",
    "stars = review['stars'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Encoders and Model\n",
    "After training and evaluation, we save the label encoders and the trained model to disk. This allows us to reload the trained model and encoders for future predictions without retraining from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(f'./model/model.weights.h5',\n",
    "                             monitor='val_loss',   # Monitor validation loss\n",
    "                             save_best_only=True,  # Save only the best model\n",
    "                             save_weights_only=True,\n",
    "                             mode='min'            # Mode of monitoring (minimize validation loss)\n",
    "                            )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=1,\n",
    "                               restore_best_weights=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - loss: 3.1757 - mae: 1.4261 - val_loss: 2.7720 - val_mae: 1.4321\n",
      "Epoch 2/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 1.4790 - mae: 0.9775 - val_loss: 2.4720 - val_mae: 1.3386\n",
      "Epoch 3/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 1.0670 - mae: 0.8126 - val_loss: 2.4136 - val_mae: 1.3080\n",
      "Epoch 4/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 0.8661 - mae: 0.7221 - val_loss: 2.4012 - val_mae: 1.2962\n",
      "Epoch 5/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.7475 - mae: 0.6651 - val_loss: 2.3900 - val_mae: 1.3017\n",
      "Epoch 6/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.6778 - mae: 0.6305 - val_loss: 2.3493 - val_mae: 1.2849\n",
      "Epoch 7/10\n",
      "\u001b[1m1528/1528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 0.6250 - mae: 0.6025 - val_loss: 2.4385 - val_mae: 1.3275\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [user_ids, business_ids],\n",
    "    stars,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model/user_encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(user_encoder, f)\n",
    "    \n",
    "with open('./model/business_encoder.pickle', 'wb') as f:\n",
    "    pickle.dump(business_encoder, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
